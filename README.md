<p align="center">
  <img src="https://img.shields.io/badge/Python-3.8+-3776AB?style=for-the-badge&logo=python&logoColor=white" alt="Python">
  <img src="https://img.shields.io/badge/PyTorch-EE4C2C?style=for-the-badge&logo=pytorch&logoColor=white" alt="PyTorch">
  <img src="https://img.shields.io/badge/CLIP-OpenAI-412991?style=for-the-badge&logo=openai&logoColor=white" alt="CLIP">
  <img src="https://img.shields.io/badge/DINOv2-Meta-0467DF?style=for-the-badge&logo=meta&logoColor=white" alt="DINOv2">
  <img src="https://img.shields.io/badge/FAISS-Facebook-4267B2?style=for-the-badge&logo=facebook&logoColor=white" alt="FAISS">
  <img src="https://img.shields.io/badge/Streamlit-FF4B4B?style=for-the-badge&logo=streamlit&logoColor=white" alt="Streamlit">
</p>

<h1 align="center">ğŸ”® DejaView</h1>
<h3 align="center">Near-Duplicate Image Detection System</h3>

<p align="center">
  <i>"Maya represents the veil of illusion where one truth can take a thousand different forms."</i>
</p>

<p align="center">
  <b>DejaView</b> is a high-performance Near-Duplicate Image Detection system that acts like the <b>Sudarshana Chakra</b>â€”a tool of ultimate discernment that cuts through the illusions of editing and compression to identify the original "soul" (the source image) within a vast sea of data.
</p>

---

## ğŸ“‹ Table of Contents

<details>
<summary>Click to expand</summary>

- [ğŸ¯ Problem Statement](#-problem-statement)
- [ğŸ’¡ Solution Overview](#-solution-overview)
- [ğŸ—ï¸ System Architecture](#ï¸-system-architecture)
- [ğŸ”„ Pipeline Workflow](#-pipeline-workflow)
- [ğŸ“ Project Structure](#-project-structure)
- [ğŸ—‚ï¸ Directory Deep Dive](#ï¸-directory-deep-dive)
- [ğŸ› ï¸ Installation](#ï¸-installation)
- [ğŸš€ Usage](#-usage)
- [ğŸ”¬ Technical Details](#-technical-details)
- [ğŸ“Š Evaluation Metrics](#-evaluation-metrics)
- [ğŸ“š Datasets](#-datasets)
- [ğŸ¬ Demo](#-demo)

</details>

---

## ğŸ¯ Problem Statement

In modern digital platforms such as **social media**, **e-commerce**, **content hosting**, and **news aggregation systems**, millions of images are uploaded every day. A significant portion of these uploads are **duplicate or near-duplicate images**â€”the same image uploaded multiple times or slightly modified versions of an existing image.

### ğŸš§ The Challenge

Traditional systems struggle to automatically detect these duplicates when images are:

<table>
<tr>
<td align="center">ğŸ”„<br><b>Resized</b></td>
<td align="center">âœ‚ï¸<br><b>Cropped</b></td>
<td align="center">ğŸ“¦<br><b>Compressed</b></td>
<td align="center">ğŸ¨<br><b>Color-adjusted</b></td>
<td align="center">ğŸ’§<br><b>Watermarked</b></td>
<td align="center">ğŸ–¼ï¸<br><b>Slightly edited</b></td>
</tr>
<tr>
<td>Scaled up/down</td>
<td>Portions removed</td>
<td>JPEG artifacts</td>
<td>Brightness, contrast</td>
<td>Text/logos overlaid</td>
<td>Filters applied</td>
</tr>
</table>

###  Why It Matters

| Use Case | Benefit |
|----------|---------|
| **ğŸ—„ï¸ Storage Optimization** | Eliminating redundant copies to save petabytes of cloud storage |
| **ğŸ›¡ï¸ Spam & Integrity** | Preventing "repost bots" from flooding feeds and protecting original creators |
| **ğŸ” Search Relevance** | Ensuring a news aggregator doesn't show the same thumbnail ten times |
| **âš–ï¸ Copyright Protection** | Identifying unauthorized use of copyrighted images |

---

## ğŸ’¡ Solution Overview

DejaView implements a **multi-layered detection pipeline** that combines:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                           ğŸ”® DejaView Detection Pipeline                            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                                     â”‚
â”‚  ğŸ“¥ Input â”€â”€â–º ğŸ¯ Feature Check â”€â”€â–º ï¿½ Hash Check â”€â”€â–º ï¿½ DINO â”€â”€â–º ğŸ“ CLIP â”€â”€â–º âœ… Out â”‚
â”‚                    â”‚ (ORB)              â”‚              â”‚           â”‚                â”‚
â”‚                    â–¼                    â–¼              â–¼           â–¼                â”‚
â”‚               [Rejected]           [Similar]       [Similar]   [Similar/Unique]     â”‚
â”‚                                                                                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

| Stage | Method | Speed | Use Case |
|-------|--------|-------|----------|
| **1ï¸âƒ£ Structure Check** | ORB Keypoints | âš¡ Fast | Early rejection of featureless images (< 20 features) |
| **2ï¸âƒ£ Hash Matching** | pHash + wHash | âš¡ Fast | Fast fingerprinting with augmentation support |
| **3ï¸âƒ£ DINO Check** | DINOv2 Embeddings | ï¿½ Thorough | Structural similarity detection |
| **4ï¸âƒ£ CLIP Check** | CLIP Embeddings | ğŸ¢ Thorough | Semantic similarity detection |

---

## ğŸ—ï¸ System Architecture

```mermaid
flowchart TB
    subgraph Input["ğŸ“¥ Input Layer"]
        UI[ğŸ–¥ï¸ Streamlit UI]
        API[ğŸ“¦ ndid_model.py]
    end

    subgraph Core["âš™ï¸ Core Processing"]
        DC[ğŸ”§ duplicate_checker.py]
        
        subgraph FeatureCheck["ğŸ¯ Feature Check"]
            ORB[ORB Keypoints â‰¥ 20]
        end
        
        subgraph Hashing["ğŸ” Hashing Layer"]
            direction TB
            PH[ğŸ“Š pHash Check]
            WH[ğŸŒŠ wHash Check]
        end
        
        subgraph Deep["ğŸ§  Deep Learning"]
            DINO[ğŸ¦• DINOv2]
            CLIP[ğŸ“ CLIP]
        end
    end

    subgraph Storage["ğŸ’¾ Vector Storage"]
        subgraph FAISS["FAISS Indices"]
            PI[(phash.index)]
            WI[(whash.index)]
            CI[(clip.index)]
            DI[(dino.index)]
        end
        
        subgraph Paths["ğŸ“ Image Mappings"]
            HP[phash_paths.npy]
            CP[clip_paths.npy]
            DP[dino_paths.npy]
        end
    end

    subgraph Models["ğŸ¤– AI Models"]
        ModelCheck{Local Available?}
        Local[ğŸ’¾ Local Models]
        Online[ğŸŒ HuggingFace]
    end

    %% Flow Connections
    UI --> API
    API --> DC
    
    %% Processing Flow
    DC --> ORB
    ORB --> PH
    PH --> WH
    WH --> DINO
    DINO --> CLIP
    
    %% Storage Lookups
    PH <--> PI
    WH <--> WI
    DINO <--> DI
    CLIP <--> CI
    
    %% Path Retrievals
    PI --> HP
    CI --> CP
    DI --> DP
    
    %% Model Loading
    CLIP --> ModelCheck
    DINO --> ModelCheck
    ModelCheck -->|Yes| Local
    ModelCheck -->|No| Online
```

---

## ğŸ”„ Pipeline Workflow

### Phase 1: Preprocessing & Indexing (Offline)

```mermaid
flowchart LR
    subgraph Preprocessing["ğŸ“¥ Preprocessing"]
        A[ğŸ–¼ï¸ Raw Images] --> B[âœ… Image Validation]
        B --> C[ğŸ”„ EXIF Transpose]
        C --> D[ğŸ¨ RGB Conversion]
        D --> E[âœ¨ Preprocessed Images]
    end
    
    subgraph Augmentation["ï¿½ Augmentation"]
        E --> AUG[Generate 7 Variants]
        AUG --> |Original, Rotate 90/180/270, Flip H/V/Both| F[ğŸ“Š Hash Each Variant]
    end
    
    subgraph Hashing["ğŸ” Hash Generation"]
        F --> G[pHash Vectors]
        F --> H[wHash Vectors]
        G --> I[ğŸ“ Binary FAISS Index]
        H --> J[ğŸ“ Binary FAISS Index]
    end
    
    subgraph Embedding["ğŸ§  Deep Embeddings"]
        E --> K[ğŸ“ CLIP Embedding]
        E --> L[ğŸ¦• DINO Embedding]
        K --> M[ğŸ“ L2 Normalize]
        L --> N[ğŸ“ L2 Normalize]
        M --> O[ğŸ“ Inner Product Index]
        N --> P[ğŸ“ Inner Product Index]
    end
```

### Phase 2: Query & Detection (Online)

```mermaid
flowchart TD
    A[ğŸ“¤ Upload Image] --> SC{ğŸ¯ Feature Check}
    SC -->|ORB < 20| R[ğŸš« REJECTED]
    SC -->|ORB â‰¥ 20| B{ğŸ“Š pHash + wHash}
    
    B -->|Both > 92%| D1[âœ… SIMILAR - Hash Match]
    B -->|No Match| DL{ğŸ¦• DINO Check}
    
    DL -->|Score â‰¥ 82%| D2[âœ… SIMILAR - DINO]
    DL -->|Score < 50%| U[âŒ UNIQUE]
    DL -->|50% â‰¤ Score < 82%| CL{ï¿½ CLIP Check}
    
    CL -->|Score â‰¥ 69%| D3[âœ… SIMILAR - CLIP]
    CL -->|Score < 69%| U
```

---

## ğŸ“ Project Structure

```
DejaView/
â”‚
â”œâ”€â”€ ğŸ¯ Core Detection Engine
â”‚   â”œâ”€â”€ ğŸ“„ streamlitUI.py          # Web interface for image upload & results (76 lines)
â”‚   â”œâ”€â”€ ğŸ“„ ndid_model.py           # Bridge between UI and detection pipeline (59 lines)
â”‚   â”œâ”€â”€ ğŸ“„ duplicate_checker.py    # Core detection logic with 4-stage pipeline (326 lines)
â”‚   â””â”€â”€ ğŸ“„ index_manager.py        # Manages FAISS shards & indices (201 lines)
â”‚
â”œâ”€â”€ ğŸ”§ Processing & Training
â”‚   â”œâ”€â”€ ğŸ“„ Final_preprocessing_hashing.py  # Image preprocessing & augmented hash generation (183 lines)
â”‚   â”œâ”€â”€ ğŸ“„ Faiss_implementation.py         # FAISS binary index creation utilities (79 lines)
â”‚   â”œâ”€â”€ ğŸ“„ clip_train.py                   # CLIP embedding logic (75 lines)
â”‚   â”œâ”€â”€ ğŸ“„ dino_train.py                   # DINOv2 embedding logic (92 lines)
â”‚   â”œâ”€â”€ ğŸ“„ download_model.py               # Interactive model downloader (64 lines)
â”‚   â””â”€â”€ ğŸ“„ download_official_dino.py       # Direct DINOv2 downloader (33 lines)
â”‚
â”œâ”€â”€ ğŸ”§ Maintenance Scripts
â”‚   â”œâ”€â”€ ğŸ“„ add_to_database.py      # Batch add images to indices (47 lines)
â”‚   â”œâ”€â”€ ğŸ“„ all_images.py           # Full directory indexing with all methods (100 lines)
â”‚   â”œâ”€â”€ ğŸ“„ remove_from_index.py    # Remove specified images from indices (143 lines)
â”‚   â”œâ”€â”€ ğŸ“„ inspect_paths.py        # Debug utility for path inspection (29 lines)
â”‚   â””â”€â”€ ğŸ“„ test_checker.py         # Quick test script for pipeline (25 lines)
â”‚
â”œâ”€â”€ ğŸ“‚ models/                      # ğŸ¤– Local Model Weights (~950MB)
â”‚   â”œâ”€â”€ ğŸ“‚ clipViTb32/             # CLIP ViT-B/32 (605MB)
â”‚   â””â”€â”€ ğŸ“‚ dinoV2b14reg/           # DINOv2 Base w/ Registers (346MB)
â”‚
â”œâ”€â”€ ğŸ“‚ index/                       # ğŸ’¾ Vector Search Indices
â”‚   â”œâ”€â”€ ğŸ“„ phash_*.index           # Perceptual Hash Shards
â”‚   â”œâ”€â”€ ğŸ“„ whash_*.index           # Wavelet Hash Shards
â”‚   â”œâ”€â”€ ğŸ“„ clip_*.index            # CLIP Embedding Shards
â”‚   â”œâ”€â”€ ğŸ“„ dino_*.index            # DINO Embedding Shards
â”‚   â””â”€â”€ ğŸ“„ *_paths.npy             # Path mappings
â”‚
â”œâ”€â”€ ğŸ“‚ final_hist/                  # ğŸ‘ï¸ Histogram Matching (Currently Unused)
â”‚   â””â”€â”€ ğŸ“„ hist_matching.py        # Histogram & ORB matching - imported but not active (179 lines)
â”‚
â”œâ”€â”€ ğŸ“‚ metrics/                     # ğŸ“Š Evaluation Suite
â”‚   â”œâ”€â”€ ğŸ“„ create_evaluation_data.py   # Generates transformed test images
â”‚   â”œâ”€â”€ ğŸ“„ add_non_matching.py         # Creates synthetic non-matching images
â”‚   â”œâ”€â”€ ğŸ“„ combine_ground_truth.py     # Merges positive/negative ground truth CSVs
â”‚   â”œâ”€â”€ ğŸ“„ evaluate.py                 # Runs full evaluation with F1/Precision/Recall (159 lines)
â”‚   â””â”€â”€ ğŸ“‚ evaluation/                 # Test data & results
â”‚
â”œâ”€â”€ ğŸ“‚ CLIP/                        # ğŸ“ Legacy CLIP indices (from older version)
â”œâ”€â”€ ğŸ“‚ assets/                      # ğŸ¨ Documentation assets
â”œâ”€â”€ ğŸ“‚ images/                      # ğŸ–¼ï¸ Dataset images
â”‚   â””â”€â”€ ğŸ“‚ uploads/                # New unique images stored here
â”‚
â”œâ”€â”€ ğŸ“‚ index_backup/                # ğŸ’¾ Index backups
â”‚
â””â”€â”€ ğŸ“„ requirements.txt             # Dependencies
```

---

## ğŸ—‚ï¸ Directory Deep Dive

<details>
<summary><b>ğŸ“‚ models/</b> â€” AI Model Weights</summary>

| Directory | Model | Size | Description |
|-----------|-------|------|-------------|
| `clipViTb32/` | CLIP ViT-B/32 | 605MB | OpenAI's vision-language model for semantic understanding |
| `dinoV2b14reg/` | DINOv2 Base | 346MB | Meta's self-supervised vision model with registers |

**Files in `clipViTb32/`:**
- `model.safetensors` â€” Model weights
- `config.json` â€” Model configuration
- `tokenizer.json` â€” Text tokenizer
- `preprocessor_config.json` â€” Image preprocessing config

**Files in `dinoV2b14reg/`:**
- `model.safetensors` â€” Model weights
- `config.json` â€” Model configuration
- `preprocessor_config.json` â€” Image preprocessing config

</details>

<details>
<summary><b>ğŸ“‚ index/</b> â€” FAISS Vector Indices</summary>

| File Pattern | Type | Description |
|--------------|------|-------------|
| `phash_*.index` | Binary Flat | Perceptual hash indices (Hamming distance) |
| `whash_*.index` | Binary Flat | Wavelet hash indices (Hamming distance) |
| `clip_*.index` | Flat IP | CLIP embedding indices (Inner Product) |
| `dino_*.index` | Flat IP | DINO embedding indices (Inner Product) |
| `*_paths.npy` | NumPy Array | Image path mappings for each index type |

**Index Features:**
- ğŸ“ **Shard Limit:** 1,000,000 vectors per shard (auto-rotates)
- ï¿½ **Augmentation:** Each image generates 7 hash variants (original + rotations + flips)

</details>

<details>
<summary><b>ğŸ“‚ final_hist/</b> â€” Histogram Matching (Currently Unused)</summary>

| File | Purpose |
|------|---------|
| `hist_matching.py` | Multi-factor image comparison using Histogram + ORB |

**Note:** This module is imported but **NOT actively used** in the pipeline. The `hist_match` verification code is commented out in `duplicate_checker.py`.

**Available Functions (for future use):**
- `hist_match()` â€” Main entry point for verification
- `get_orb_score()` â€” ORB feature matching
- `get_histogram_score()` â€” Color/structure histogram comparison
- `get_feature_count()` â€” **ACTIVE** - Used for structure check (rejects images with < 20 ORB features)

**Weights (if enabled):**
```python
{
    'structure': 0.42,  # Grayscale histogram
    'spatial':   0.43,  # ORB keypoint matches
    'color':     0.15   # HSV histogram
}
```

</details>

<details>
<summary><b>ğŸ“‚ metrics/</b> â€” Evaluation Suite</summary>

| Script | Purpose |
|--------|---------|
| `create_evaluation_data.py` | Generates transformed test images (resize, crop, etc.) |
| `add_non_matching.py` | Creates synthetic non-matching images |
| `combine_ground_truth.py` | Merges positive/negative ground truth CSVs |
| `evaluate.py` | Runs full evaluation and calculates F1/Precision/Recall |

**Output Files in `evaluation/`:**
- `ground_truth.csv` â€” Combined ground truth
- `detailed_results.csv` â€” Per-image results with TP/TN/FP/FN status

</details>

---

## ğŸ§© Module Descriptions

### Core Modules

| Module | Lines | Purpose |
|--------|-------|---------|
| `duplicate_checker.py` | 326 | ğŸ”§ **Main Engine**: Coordinates the 4-stage detection pipeline |
| `index_manager.py` | 201 | ğŸ’¾ **Shard Manager**: Handles FAISS index loading, saving, searching, and auto-rotation |
| `ndid_model.py` | 59 | ğŸ”— **Bridge**: Handles file upload, invokes detection, saves unique images to uploads/ |
| `streamlitUI.py` | 76 | ğŸ–¥ï¸ **UI**: Interactive web interface with side-by-side comparison |

### Supporting Modules

| Module | Lines | Purpose |
|--------|-------|---------|
| `clip_train.py` | 75 | ğŸ“ CLIP embedding generation with local/online fallback |
| `dino_train.py` | 92 | ğŸ¦• DINOv2 embedding generation with local/online fallback |
| `Final_preprocessing_hashing.py` | 183 | ğŸ” pHash/wHash computation with 7-variant augmentation |
| `Faiss_implementation.py` | 79 | ğŸ“ FAISS binary index creation and hash-to-vector conversion |
| `hist_matching.py` | 179 | ğŸ‘ï¸ Histogram + ORB verification (currently unused in pipeline) |

### Utility Scripts

| Script | Lines | Purpose |
|--------|-------|---------|
| `add_to_database.py` | 47 | Batch add images from `images/` folder to all indices |
| `all_images.py` | 100 | Full directory scan with augmented hashes + deep embeddings |
| `remove_from_index.py` | 143 | Remove specified images from indices with backup |
| `download_model.py` | 64 | Interactive download for CLIP and/or DINO models |
| `test_checker.py` | 25 | Quick test with koala.png test image |

---

## ğŸ› ï¸ Installation

### Prerequisites

- Python 3.8+
- pip or conda
- ~2GB disk space for models

### Quick Start

```bash
# 1. Clone the repository
git clone https://github.com/your-username/DejaView.git
cd DejaView

# 2. Create virtual environment (recommended)
python -m venv venv
source venv/bin/activate  # Linux/Mac
# or
.\venv\Scripts\activate   # Windows

# 3. Install dependencies
pip install -r requirements.txt

# 4. Download models (one-time)
python download_model.py
```

### Dependencies

```
imagehash        # Perceptual & wavelet hashing
faiss-cpu        # Vector similarity search
numpy            # Numerical operations
Pillow           # Image processing
transformers     # CLIP & DINO model loading
torch            # Deep learning backend
streamlit        # Web UI framework
opencv-python    # Computer Vision (ORB feature detection)
```

---

## ğŸš€ Usage

### ğŸ–¥ï¸ Web Interface

```bash
streamlit run streamlitUI.py
```

This launches a local web server (typically at `http://localhost:8501`) where you can:

1. **ğŸ“¤ Upload an image** (JPG, PNG, BMP, WEBP supported)
2. **ğŸ” Click "Run NDID"** to analyze
3. **ğŸ“Š View results**: Status, similarity percentage, method used, and matched image

### ğŸ’» Programmatic Usage

```python
from duplicate_checker import check_image_pipeline

result = check_image_pipeline("path/to/your/image.jpg")

print(result)
# {
#     "status": "Similar",           # Unique | Similar | Rejected
#     "similarity_percentage": 87.5,
#     "matched_image_path": "/path/to/matched_image.png",
#     "source_image_path": "path/to/your/image.jpg",
#     "method": "DINO"               # phash & whash | DINO | CLIP | insufficient_features
# }
```

### ğŸ”§ Maintenance Commands

```bash
# Add all images from images/ folder to database
python add_to_database.py

# Full indexing with augmentation (recommended for initial setup)
python all_images.py

# Remove specific images from indices
# (Edit REMOVE_PATHS list in the script first)
python remove_from_index.py

# Quick pipeline test
python test_checker.py
```

---

## ğŸ”¬ Technical Details

### ğŸ›ï¸ Detection Thresholds

| Method | Threshold | Metric | Description |
|--------|-----------|--------|-------------|
| **Structure Check** | â‰¥ 20 | ORB Keypoints | Minimum feature count to proceed |
| **pHash + wHash** | Both > 92% | Similarity % | Match if both hashes agree strongly |
| **pHash/wHash** | â‰¤ 4 bits | Hamming Distance | Individual hash match threshold |
| **DINO** | â‰¥ 0.82 | Cosine Similarity | Structural similarity check |
| **CLIP** | â‰¥ 0.69 | Cosine Similarity | Semantic similarity check |

### ğŸ”„ Image Augmentation for Indexing

Each image is indexed with **7 variants** to handle rotations and flips:

```python
{
    "original": img,
    "rotate_90": img.rotate(270, expand=True),
    "rotate_180": img.rotate(180, expand=True),
    "rotate_270": img.rotate(90, expand=True),
    "flip_horizontal": img.transpose(FLIP_LEFT_RIGHT),
    "flip_vertical": img.transpose(FLIP_TOP_BOTTOM),
    "flip_both": img.transpose(FLIP_TOP_BOTTOM).transpose(FLIP_LEFT_RIGHT),
}
```

### ğŸ§  Model Specifications

<table>
<tr>
<th>Model</th>
<th>CLIP ViT-B/32</th>
<th>DINOv2 Base</th>
</tr>
<tr>
<td><b>Architecture</b></td>
<td>Vision Transformer</td>
<td>Vision Transformer + Registers</td>
</tr>
<tr>
<td><b>Embedding Dim</b></td>
<td>512</td>
<td>768</td>
</tr>
<tr>
<td><b>Input Size</b></td>
<td>224Ã—224</td>
<td>224Ã—224</td>
</tr>
<tr>
<td><b>Source</b></td>
<td>OpenAI via HuggingFace</td>
<td>Meta via HuggingFace</td>
</tr>
<tr>
<td><b>Strength</b></td>
<td>Semantic understanding</td>
<td>Structural features</td>
</tr>
</table>

### ğŸ“ FAISS Index Types

| Index | Type | Use Case |
|-------|------|----------|
| `phash_*.index` | `IndexBinaryFlat` | Exact Hamming distance search |
| `whash_*.index` | `IndexBinaryFlat` | Exact Hamming distance search |
| `clip_*.index` | `IndexFlatIP` | Inner product (cosine) search |
| `dino_*.index` | `IndexFlatIP` | Inner product (cosine) search |

### ğŸ”„ Pipeline Logic Summary

```
1. Structure Check: get_feature_count() â‰¥ 20 ORB features
   â””â”€ FAIL â†’ Status: "Rejected", method: "insufficient_features"

2. Hash Check: pHash AND wHash both > 92%
   â””â”€ PASS â†’ Status: "Similar (pHash & wHash)"

3. DINO Check: Score â‰¥ 82%
   â””â”€ PASS â†’ Status: "Similar", method: "DINO"
   â””â”€ Score < 50% â†’ Status: "Unique"

4. CLIP Check: Score â‰¥ 69%
   â””â”€ PASS â†’ Status: "Similar", method: "CLIP"
   â””â”€ FAIL â†’ Status: "Unique"
```

---

## ğŸ“Š Evaluation Metrics

The system is evaluated using standard information retrieval metrics:

```
Precision = TP / (TP + FP)    â€” Of detected duplicates, how many are correct?
Recall    = TP / (TP + FN)    â€” Of actual duplicates, how many did we find?
F1 Score  = 2 Ã— (P Ã— R)/(P + R)   â€” Harmonic mean of precision and recall
```

### ğŸ“ˆ Running Evaluation

```bash
# 1. Generate positive test cases (transformed images)
python metrics/create_evaluation_data.py

# 2. Generate negative test cases (unique images)
python metrics/add_non_matching.py

# 3. Combine ground truth data
python metrics/combine_ground_truth.py

# 4. Run evaluation
python metrics/evaluate.py
```

**Output:**
- Console: F1 Score, Confusion Matrix, Per-transform accuracy
- File: `evaluation/detailed_results.csv`

---

## ğŸ“š Datasets

### Recommended Datasets

| Dataset | Description | Link |
|---------|-------------|------|
| **Google Landmarks V2** | 5M+ landmark images with near-duplicates | [GitHub](https://github.com/cvdfoundation/google-landmark) |
| **INRIA Copydays** | Benchmark for copy detection with distortions | [INRIA](https://thoth.inrialpes.fr/~jegou/data.php.html#copydays) |
| **California-ND** | Near-duplicate detection benchmark | [Paper](https://dl.acm.org/doi/10.1145/2911996.2912036) |

---

## ğŸ›ï¸ Architecture Summary

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                              ğŸ”® DejaView System                                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚   Frontend  â”‚     â”‚              Backend Pipeline                          â”‚  â”‚
â”‚  â”‚             â”‚     â”‚                                                        â”‚  â”‚
â”‚  â”‚  Streamlit  â”‚â”€â”€â”€â”€â–ºâ”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”  â”‚  â”‚
â”‚  â”‚     UI      â”‚     â”‚  â”‚ Feature â”‚â”€â”€â–ºâ”‚  Hash   â”‚â”€â”€â–ºâ”‚  DINO   â”‚â”€â”€â–ºâ”‚ CLIP  â”‚  â”‚  â”‚
â”‚  â”‚             â”‚     â”‚  â”‚ Check   â”‚   â”‚ Check   â”‚   â”‚  Check  â”‚   â”‚ Check â”‚  â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”¬â”€â”€â”€â”˜  â”‚  â”‚
â”‚                      â”‚       â”‚             â”‚              â”‚            â”‚      â”‚  â”‚
â”‚                      â”‚       â–¼             â–¼              â–¼            â–¼      â”‚  â”‚
â”‚                      â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚  â”‚
â”‚                      â”‚  â”‚              FAISS Vector Store                  â”‚   â”‚  â”‚
â”‚                      â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚   â”‚  â”‚
â”‚                      â”‚  â”‚  â”‚ phash   â”‚ â”‚ whash   â”‚ â”‚  dino   â”‚ â”‚  clip   â”‚â”‚   â”‚  â”‚
â”‚                      â”‚  â”‚  â”‚ .index  â”‚ â”‚ .index  â”‚ â”‚ .index  â”‚ â”‚ .index  â”‚â”‚   â”‚  â”‚
â”‚                      â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â”‚   â”‚  â”‚
â”‚                      â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚  â”‚
â”‚                      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                                                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ¬ Demo

<p align="center">
  <a href="https://www.youtube.com/watch?v=YOUR_VIDEO_ID">
    <img src="https://img.shields.io/badge/YouTube-Watch%20Demo-FF0000?style=for-the-badge&logo=youtube&logoColor=white" alt="Watch Demo on YouTube">
  </a>
</p>

---

## ğŸ‘¥ Authors

Built with â¤ï¸ as part of the NDID (Near-Duplicate Image Detection) project.

---

## ğŸ“œ License

This project is for educational purposes.

---

<p align="center">
  <i>"Through the veil of Maya, DejaView sees the truth."</i>
</p>

<p align="center">
  <img src="https://img.shields.io/badge/Made%20with-Python-3776AB?style=flat&logo=python&logoColor=white">
  <img src="https://img.shields.io/badge/Powered%20by-FAISS-4267B2?style=flat&logo=facebook&logoColor=white">
  <img src="https://img.shields.io/badge/AI-CLIP%20%2B%20DINOv2-FF6F00?style=flat&logo=tensorflow&logoColor=white">
</p>